---
title:          "Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models"
date:           2024-05-12 00:01:00 +0800
selected:       true
pub:            "Preprint"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       ' <span class="badge badge-pill badge-publication badge-danger">Arxiv</span>'
pub_date:       "2024"

abstract: >-
  Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models.
  In this work, we propose Diffusion-of-Thought (DoT),  a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models. In contrast to autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT allows reasoning steps to diffuse over time through a diffusion language model and offers greater flexibility in trading-off computation for reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication, boolean logic, and grade school math problems, with a small diffusion model outperforming a much larger autoregressive model in both efficiency and accuracy. In addition to that, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning with diffusion language models.
# cover:          assets/images/covers/HiddenKey.jpg
authors:
  - Jiacheng Ye*
  - Shansan Gong*
  - Liheng Chen*
  - Lin Zheng
  - Jiahui Gao
  - Han Shi
  - Chuan Wu
  - Xin Jiang
  - Zhenguo Li
  - Wei Bi
  - Lingpeng Kong

links:
  Paper: https://arxiv.org/abs/2402.07754
  Code: https://github.com/HKUNLP/diffusion-of-thoughts
# Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk
---
